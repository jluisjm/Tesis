{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob, Word\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def clean_corpus(corpus):\n",
    "  xcorpus = corpus.get_values()\n",
    "  for i in range(len(corpus)):\n",
    "    xcorpus[i] = re.sub(\"[^a-zA-Z]\", \" \", corpus[i].lower())\n",
    "    xcorpus[i] = ' '.join(xcorpus[i].split())\n",
    "  return xcorpus\n",
    "\n",
    "snowball = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n",
    "\n",
    "def tokenize(text):\n",
    "    return [snowball.stem(word) for word in tokenizer.tokenize(text.lower())]\n",
    "\n",
    "def split_into_lemmas(text):\n",
    "    text = unicode(text, 'utf-8').lower()\n",
    "    words = TextBlob(text).words\n",
    "    return [word.lemmatize() for word in words]\n",
    "\n",
    "data = pd.read_csv(\"/home/jluis2/datosbi.csv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "x = data['review']\n",
    "y = data.label\n",
    "corpus = clean_corpus(x)\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, sublinear_tf=True, norm='l2', ngram_range=(1, 2), \\\n",
    "                       max_features=30000, min_df=5, stop_words='english', use_idf=True)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(i, clf):\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    t0 = time()\n",
    "    scores = cross_val_score(clf, X, y, cv=kf)\n",
    "    train_time = time() - t0\n",
    "    avg_score = np.mean(scores)\n",
    "    minv = np.ndarray.min(scores)\n",
    "    maxv = np.ndarray.max(scores)\n",
    "    mse = cross_val_score(clf, X, y, cv=kf,  scoring='neg_mean_squared_error')\n",
    "    avg_mse = np.mean(mse) * -1\n",
    "    print('=' * 80)\n",
    "    print(clf)\n",
    "    print(\"{0}. AVG: {1:.4f}, MIN: {2:.4f}, MAX: {3:.4f}, TIME: {4:.3f}, AVGMSE: {5:.4f}\".format(i, avg_score, \\\n",
    "        minv, maxv, train_time, avg_mse))\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=50, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "1. AVG: 0.8256, MIN: 0.8151, MAX: 0.8377, TIME: 8329.008, AVGMSE: 2.7989\n",
      "================================================================================\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "2. AVG: 0.8256, MIN: 0.8201, MAX: 0.8298, TIME: 8617.799, AVGMSE: 2.7924\n",
      "================================================================================\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.05, max_iter=50, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "3. AVG: 0.8404, MIN: 0.8326, MAX: 0.8458, TIME: 4495.533, AVGMSE: 2.5184\n",
      "================================================================================\n",
      "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "4. AVG: 0.8404, MIN: 0.8323, MAX: 0.8550, TIME: 3799.205, AVGMSE: 2.5355\n",
      "================================================================================\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "5. AVG: 0.8382, MIN: 0.8323, MAX: 0.8479, TIME: 4447.397, AVGMSE: 2.5815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.0003, max_iter=200, momentum=0.98,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='sgd', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "6. AVG: 0.8716, MIN: 0.8663, MAX: 0.8761, TIME: 52515.334, AVGMSE: 2.0568\n",
      "================================================================================\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.0001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "7. AVG: 0.8273, MIN: 0.8208, MAX: 0.8326, TIME: 24592.469, AVGMSE: 2.7714\n"
     ]
    }
   ],
   "source": [
    "results.append(benchmark(1, MLPClassifier(activation='relu', solver='adam', learning_rate_init=0.001, max_iter=50)))\n",
    "results.append(benchmark(2, MLPClassifier(activation='relu', solver='adam', learning_rate_init=0.001, max_iter=100)))\n",
    "results.append(benchmark(3, MLPClassifier(activation='relu', solver='adam', learning_rate_init=0.05, max_iter=50)))\n",
    "results.append(benchmark(4, MLPClassifier(activation='tanh', solver='lbfgs')))\n",
    "results.append(benchmark(5, MLPClassifier(solver='adam', learning_rate_init=0.001, hidden_layer_sizes=(100,100))))\n",
    "results.append(benchmark(6, MLPClassifier(solver='sgd', learning_rate_init=0.0003, momentum=0.98)))\n",
    "results.append(benchmark(7, MLPClassifier(solver='adam', learning_rate_init=0.0001)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
      "       learning_rate_init=0.0001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='sgd', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "8. AVG: 0.6000, MIN: 0.5939, MAX: 0.6031, TIME: 2572.391, AVGMSE: 6.4000\n"
     ]
    }
   ],
   "source": [
    "rprint chkesults.append(benchmark(8, MLPClassifier(solver='sgd', learning_rate_init=0.0001, hidden_layer_sizes=(100,100))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "results.append(benchmark(9, MLPClassifier(activation='relu', max_iter=25, batch_size=5)))\n",
    "results.append(benchmark(10, MLPClassifier(solver='adam', alpha=0.001, hidden_layer_sizes=(100,100), random_state=1, activation='logistic', early_stopping=True)))\n",
    "results.append(benchmark(11, MLPClassifier(solver='adam', alpha=0.001, hidden_layer_sizes=(100,100), activation='logistic')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
